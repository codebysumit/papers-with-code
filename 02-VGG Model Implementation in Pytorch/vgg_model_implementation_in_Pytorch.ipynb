{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "VGG Paper: [Very Deep Convolutional Networks for Large-Scale Image Recognition.]()"
      ],
      "metadata": {
        "id": "gII0Z40BYfGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source code for [torchvision.models.vgg](https://docs.pytorch.org/vision/0.8/_modules/torchvision/models/vgg.html)\n",
        "\n",
        "vgg16: https://download.pytorch.org/models/vgg16-397923af.pth\n",
        "\n",
        "vgg16_bn: https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\n"
      ],
      "metadata": {
        "id": "xNt4Pcs1ds80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dependences"
      ],
      "metadata": {
        "id": "QCpUlozxnfNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "Z2SMkXi1oex-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1243582a-0448-4228-e7d2-59caf2adacff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "pR9eMOREmwPQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules import linear\n",
        "\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torchvision import transforms\n",
        "\n",
        "from torchinfo import summary\n",
        "# from torchsummary import summary\n",
        "\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "# import os\n",
        "# import glob\n",
        "# import cv2\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG Models (Dynamic)"
      ],
      "metadata": {
        "id": "w-bMF7Oxnk6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class ModelClassName(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(VGG16, self).__init__()\n",
        "#     pass\n",
        "#   def forward(self, x):\n",
        "#     pass"
      ],
      "metadata": {
        "id": "ibu3f__9nkeR"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_architecture = [\n",
        "    # Block 1\n",
        "    {\"type\": \"conv2d\", \"name\": \"block1-conv1\", \"in_channels\": 3, \"out_channels\": 64, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n",
        "    {\"type\": \"conv2d\", \"name\": \"block1-conv2\", \"out_channels\": 64, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n",
        "    {\"type\": \"maxpool2d\", \"name\": \"block1-maxpool\", \"kernel_size\": 2, \"stride\": 2},\n",
        "\n",
        "    # Block 2\n",
        "    {\"type\": \"conv2d\", \"name\": \"block2-conv1\", \"out_channels\": 128, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n",
        "    {\"type\": \"conv2d\", \"name\": \"block2-conv2\", \"out_channels\": 128, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n",
        "    {\"type\": \"maxpool2d\", \"name\": \"block2-maxpool\", \"kernel_size\": 2, \"stride\": 2},\n",
        "\n",
        "    # Block 3\n",
        "    {\"type\": \"conv2d\", \"name\": \"block3-conv1\", \"out_channels\": 256, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n",
        "    {\"type\": \"conv2d\", \"name\": \"block3-conv2\", \"out_channels\": 256, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n",
        "    {\"type\": \"conv2d\", \"name\": \"block3-conv3\", \"out_channels\": 256, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n",
        "    {\"type\": \"maxpool2d\", \"name\": \"block3-maxpool\", \"kernel_size\": 2, \"stride\": 2},\n",
        "\n",
        "    # Block 4\n",
        "    {\"type\": \"conv2d\", \"name\": \"block4-conv1\", \"out_channels\": 512, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n",
        "    {\"type\": \"conv2d\", \"name\": \"block4-conv2\", \"out_channels\": 512, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n",
        "    {\"type\": \"conv2d\", \"name\": \"block4-conv3\", \"out_channels\": 512, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n",
        "    {\"type\": \"maxpool2d\", \"name\": \"block4-maxpool\", \"kernel_size\": 2, \"stride\": 2},\n",
        "\n",
        "    # Block 5\n",
        "    {\"type\": \"conv2d\", \"name\": \"block5-conv1\", \"out_channels\": 512, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n",
        "    {\"type\": \"conv2d\", \"name\": \"block5-conv2\", \"out_channels\": 512, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n",
        "    {\"type\": \"conv2d\", \"name\": \"block5-conv3\", \"out_channels\": 512, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1},\n",
        "    {\"type\": \"maxpool2d\", \"name\": \"block5-maxpool\", \"kernel_size\": 2, \"stride\": 2},\n",
        "\n",
        "    # Fully connected layers\n",
        "    {\"type\": \"flatten\"},\n",
        "    {\"type\": \"linear\", \"name\": \"fc1\", \"out_features\": 4096, \"activation\": \"relu\"},\n",
        "    {\"type\": \"linear\", \"name\": \"fc2\", \"out_features\": 4096, \"activation\": \"relu\"},\n",
        "    {\"type\": \"linear\", \"name\": \"fc3\", \"out_features\": 1000, \"activation\": \"softmax\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "7Mo7fmCT-PCR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGConvBlock(nn.Module):\n",
        "  def __init__(self, architecture, in_channels=3):\n",
        "    super(VGGConvBlock, self).__init__()\n",
        "    # class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "    #                       dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "    self.in_channels = in_channels\n",
        "    self.conv_layers = self.create_conv_layers(architecture)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv_layers(x)\n",
        "\n",
        "  def create_conv_layers(self, architecture):\n",
        "    layers = OrderedDict()\n",
        "    in_channels = self.in_channels\n",
        "\n",
        "    conv_idx = 1\n",
        "    pool_idx = 1\n",
        "\n",
        "    for cfg in architecture:\n",
        "      layer_type = cfg.get(\"type\", \"\").lower()\n",
        "\n",
        "      if layer_type == \"conv2d\":\n",
        "        if (cfg.get(\"in_channels\") is not None) and (cfg[\"in_channels\"] != in_channels):\n",
        "          raise ValueError(\n",
        "            f\"in_channels mismatch: expected {in_channels}, got {cfg['in_channels']}\"\n",
        "          )\n",
        "\n",
        "        base_name = cfg.get(\"name\", f\"conv2d{conv_idx}\")\n",
        "        layers[f\"{base_name}\"] = nn.Conv2d(in_channels=in_channels, out_channels=cfg[\"out_channels\"],\n",
        "                                           kernel_size=cfg[\"kernel_size\"], stride=cfg[\"stride\"], padding=cfg[\"padding\"])\n",
        "        layers[f\"{base_name}_batchnorm2d\"] = nn.BatchNorm2d(num_features=cfg[\"out_channels\"])\n",
        "        layers[f\"{base_name}_relu\"] = nn.ReLU(inplace=True)\n",
        "\n",
        "        in_channels = cfg[\"out_channels\"]\n",
        "        conv_idx += 1\n",
        "\n",
        "      elif cfg[\"type\"] == \"maxpool2d\":\n",
        "        base_name = cfg.get(\"name\", f\"maxpool2d{pool_idx}\")\n",
        "        layers[f\"{base_name}\"] = nn.MaxPool2d(kernel_size=cfg[\"kernel_size\"], stride=cfg[\"stride\"])\n",
        "        pool_idx += 1\n",
        "\n",
        "    return nn.Sequential(layers)\n"
      ],
      "metadata": {
        "id": "_umr8msN9QNf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = VGGConvBlock(architecture=vgg16_architecture, in_channels=3)\n",
        "# # print(model)\n",
        "# summary(model, (1, 3, 224, 224)) # for torchinfo -> summary\n",
        "# # summary(model, (3, 224, 224)) # for torchsummary -> summary\n",
        "\n",
        "# # for name, param in model.named_parameters():\n",
        "# #     print(name)"
      ],
      "metadata": {
        "id": "nfIu_lspmWZZ",
        "collapsed": true
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "  def __init__(self, architecture, in_channels=3, input_size=(224, 224)):\n",
        "    super(VGG, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.input_size = input_size # This is (H, W)\n",
        "\n",
        "    # Store the full architecture for reference if needed, but primarily split it.\n",
        "    self.full_architecture = architecture\n",
        "\n",
        "    # Correctly split architecture into conv and FC parts\n",
        "    conv_architecture = []\n",
        "    fc_architecture = []\n",
        "    in_fc_block = False # Flag to indicate if we've reached the FC block\n",
        "    for cfg in architecture:\n",
        "      if cfg.get(\"type\", \"\").lower() == \"flatten\":\n",
        "        in_fc_block = True # From now on, layers belong to FC block\n",
        "\n",
        "      if in_fc_block:\n",
        "        fc_architecture.append(cfg)\n",
        "      else:\n",
        "        conv_architecture.append(cfg)\n",
        "\n",
        "    # Initialize convolutional blocks using the VGGConvBlock class\n",
        "    # VGGConvBlock expects only the convolutional/pooling part of the architecture\n",
        "    self.conv_blocks = VGGConvBlock(architecture=conv_architecture, in_channels=in_channels)\n",
        "\n",
        "    # Initialize fully connected layers\n",
        "    # Pass the FC part of the architecture and the conv_architecture (for sizing)\n",
        "    self.fc_layers = self.create_fc_layers(fc_architecture, conv_architecture)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_blocks(x)\n",
        "    # The output of conv_blocks is typically a 4D tensor (N, C, H, W)\n",
        "    # The first layer in fc_layers will be Flatten, so it handles the flattening.\n",
        "    x = self.fc_layers(x)\n",
        "    return x\n",
        "\n",
        "  def create_fc_layers(self, fc_architecture, conv_architecture_for_sizing):\n",
        "    layers = OrderedDict()\n",
        "\n",
        "    # Calculate the input features for the first linear layer\n",
        "    # compute_fc_input_size expects (architecture_list, (C, H, W))\n",
        "    initial_fc_input_features, _ = VGG.compute_fc_input_size(\n",
        "        architecture=conv_architecture_for_sizing, # Pass the list of conv configurations (list of dicts)\n",
        "        input_size=(self.in_channels, self.input_size[0], self.input_size[1]) # Correctly pass (C, H, W)\n",
        "    )\n",
        "    in_features = initial_fc_input_features\n",
        "\n",
        "    linear_idx = 1\n",
        "\n",
        "    for cfg in fc_architecture:\n",
        "      layer_type = cfg.get(\"type\", \"\").lower()\n",
        "\n",
        "      if layer_type == \"flatten\":\n",
        "        layers[f\"flatten\"] = nn.Flatten() # Flatten layer before FC layers\n",
        "      elif layer_type == \"linear\":\n",
        "        base_name = cfg.get(\"name\", f\"linear{linear_idx}\")\n",
        "        # The first linear layer uses the calculated 'in_features'\n",
        "        layers[f\"{base_name}\"] = nn.Linear(in_features, cfg[\"out_features\"])\n",
        "\n",
        "        if cfg.get(\"activation\", \"\").lower() == \"relu\":\n",
        "          layers[f\"{base_name}_relu\"] = nn.ReLU(inplace=True)\n",
        "          layers[f\"{base_name}_dropout\"] = nn.Dropout(p=0.5)\n",
        "\n",
        "        elif cfg.get(\"activation\", \"\").lower() == \"softmax\":\n",
        "          layers[f\"{base_name}_softmax\"] = nn.Softmax(dim=1)\n",
        "\n",
        "        in_features = cfg[\"out_features\"] # Update in_features for subsequent linear layers\n",
        "        linear_idx += 1\n",
        "\n",
        "    return nn.Sequential(layers)\n",
        "\n",
        "  @staticmethod\n",
        "  def compute_fc_input_size(architecture, input_size):\n",
        "    \"\"\"\n",
        "    architecture : list of layer configs (specifically the convolutional/pooling part)\n",
        "    input_size   : tuple (C, H, W) - Initial input size to the convolutional block\n",
        "    returns: flattened feature size (C * H * W) and feature map (C, H, W)\n",
        "    \"\"\"\n",
        "    C, H, W = input_size # This line is now safe because input_size will be (C, H, W)\n",
        "\n",
        "    # Iterate through the convolutional and pooling layers to track feature map size\n",
        "    for cfg in architecture:\n",
        "        layer_type = cfg[\"type\"].lower()\n",
        "\n",
        "        if layer_type == \"conv2d\":\n",
        "            # Update channels\n",
        "            C = cfg[\"out_channels\"]\n",
        "\n",
        "            k = cfg[\"kernel_size\"]\n",
        "            s = cfg[\"stride\"]\n",
        "            p = cfg[\"padding\"]\n",
        "\n",
        "            H = math.floor((H + 2*p - k) / s) + 1\n",
        "            W = math.floor((W + 2*p - k) / s) + 1\n",
        "\n",
        "        elif layer_type == \"maxpool2d\":\n",
        "            k = cfg[\"kernel_size\"]\n",
        "            s = cfg[\"stride\"]\n",
        "\n",
        "            H = math.floor((H - k) / s) + 1\n",
        "            W = math.floor((W - k) / s) + 1\n",
        "        # No need to handle 'flatten' here, as this method only receives conv/pool architecture\n",
        "    return C * H * W, (C, H, W)"
      ],
      "metadata": {
        "id": "pM_FwmHDneLc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG(architecture=vgg16_architecture, in_channels=3, input_size=(224, 224))\n",
        "# print(model)\n",
        "summary(model, (1, 3, 224, 224)) # for torchinfo -> summary\n",
        "# summary(model, (3, 224, 224)) # for torchsummary -> summary"
      ],
      "metadata": {
        "id": "AsJForSD41zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "174ed565-3e35-4b66-e888-fea294e70cc9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG                                      [1, 1000]                 --\n",
              "├─VGGConvBlock: 1-1                      [1, 512, 7, 7]            --\n",
              "│    └─Sequential: 2-1                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-1                  [1, 64, 224, 224]         1,792\n",
              "│    │    └─BatchNorm2d: 3-2             [1, 64, 224, 224]         128\n",
              "│    │    └─ReLU: 3-3                    [1, 64, 224, 224]         --\n",
              "│    │    └─Conv2d: 3-4                  [1, 64, 224, 224]         36,928\n",
              "│    │    └─BatchNorm2d: 3-5             [1, 64, 224, 224]         128\n",
              "│    │    └─ReLU: 3-6                    [1, 64, 224, 224]         --\n",
              "│    │    └─MaxPool2d: 3-7               [1, 64, 112, 112]         --\n",
              "│    │    └─Conv2d: 3-8                  [1, 128, 112, 112]        73,856\n",
              "│    │    └─BatchNorm2d: 3-9             [1, 128, 112, 112]        256\n",
              "│    │    └─ReLU: 3-10                   [1, 128, 112, 112]        --\n",
              "│    │    └─Conv2d: 3-11                 [1, 128, 112, 112]        147,584\n",
              "│    │    └─BatchNorm2d: 3-12            [1, 128, 112, 112]        256\n",
              "│    │    └─ReLU: 3-13                   [1, 128, 112, 112]        --\n",
              "│    │    └─MaxPool2d: 3-14              [1, 128, 56, 56]          --\n",
              "│    │    └─Conv2d: 3-15                 [1, 256, 56, 56]          295,168\n",
              "│    │    └─BatchNorm2d: 3-16            [1, 256, 56, 56]          512\n",
              "│    │    └─ReLU: 3-17                   [1, 256, 56, 56]          --\n",
              "│    │    └─Conv2d: 3-18                 [1, 256, 56, 56]          590,080\n",
              "│    │    └─BatchNorm2d: 3-19            [1, 256, 56, 56]          512\n",
              "│    │    └─ReLU: 3-20                   [1, 256, 56, 56]          --\n",
              "│    │    └─Conv2d: 3-21                 [1, 256, 56, 56]          590,080\n",
              "│    │    └─BatchNorm2d: 3-22            [1, 256, 56, 56]          512\n",
              "│    │    └─ReLU: 3-23                   [1, 256, 56, 56]          --\n",
              "│    │    └─MaxPool2d: 3-24              [1, 256, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-25                 [1, 512, 28, 28]          1,180,160\n",
              "│    │    └─BatchNorm2d: 3-26            [1, 512, 28, 28]          1,024\n",
              "│    │    └─ReLU: 3-27                   [1, 512, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-28                 [1, 512, 28, 28]          2,359,808\n",
              "│    │    └─BatchNorm2d: 3-29            [1, 512, 28, 28]          1,024\n",
              "│    │    └─ReLU: 3-30                   [1, 512, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-31                 [1, 512, 28, 28]          2,359,808\n",
              "│    │    └─BatchNorm2d: 3-32            [1, 512, 28, 28]          1,024\n",
              "│    │    └─ReLU: 3-33                   [1, 512, 28, 28]          --\n",
              "│    │    └─MaxPool2d: 3-34              [1, 512, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-35                 [1, 512, 14, 14]          2,359,808\n",
              "│    │    └─BatchNorm2d: 3-36            [1, 512, 14, 14]          1,024\n",
              "│    │    └─ReLU: 3-37                   [1, 512, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-38                 [1, 512, 14, 14]          2,359,808\n",
              "│    │    └─BatchNorm2d: 3-39            [1, 512, 14, 14]          1,024\n",
              "│    │    └─ReLU: 3-40                   [1, 512, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-41                 [1, 512, 14, 14]          2,359,808\n",
              "│    │    └─BatchNorm2d: 3-42            [1, 512, 14, 14]          1,024\n",
              "│    │    └─ReLU: 3-43                   [1, 512, 14, 14]          --\n",
              "│    │    └─MaxPool2d: 3-44              [1, 512, 7, 7]            --\n",
              "├─Sequential: 1-2                        [1, 1000]                 --\n",
              "│    └─Flatten: 2-2                      [1, 25088]                --\n",
              "│    └─Linear: 2-3                       [1, 4096]                 102,764,544\n",
              "│    └─ReLU: 2-4                         [1, 4096]                 --\n",
              "│    └─Dropout: 2-5                      [1, 4096]                 --\n",
              "│    └─Linear: 2-6                       [1, 4096]                 16,781,312\n",
              "│    └─ReLU: 2-7                         [1, 4096]                 --\n",
              "│    └─Dropout: 2-8                      [1, 4096]                 --\n",
              "│    └─Linear: 2-9                       [1, 1000]                 4,097,000\n",
              "│    └─Softmax: 2-10                     [1, 1000]                 --\n",
              "==========================================================================================\n",
              "Total params: 138,365,992\n",
              "Trainable params: 138,365,992\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 15.48\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 216.83\n",
              "Params size (MB): 553.46\n",
              "Estimated Total Size (MB): 770.90\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG16 Model (Static)"
      ],
      "metadata": {
        "id": "PERxDRnwLUGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer9 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer10 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer11 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer12 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer13 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(7*7*512, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.layer9(out)\n",
        "        out = self.layer10(out)\n",
        "        out = self.layer11(out)\n",
        "        out = self.layer12(out)\n",
        "        out = self.layer13(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "yaDFWjALIeNQ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG16(num_classes=1000)\n",
        "# print(model)\n",
        "summary(model, (1, 3, 224, 224)) # for torchinfo -> summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bMdM5WQ8IwaD",
        "outputId": "d84c7f5b-0434-40d6-e866-6ec4ec910f63"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG16                                    [1, 1000]                 --\n",
              "├─Sequential: 1-1                        [1, 64, 224, 224]         --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
              "│    └─BatchNorm2d: 2-2                  [1, 64, 224, 224]         128\n",
              "│    └─ReLU: 2-3                         [1, 64, 224, 224]         --\n",
              "├─Sequential: 1-2                        [1, 64, 112, 112]         --\n",
              "│    └─Conv2d: 2-4                       [1, 64, 224, 224]         36,928\n",
              "│    └─BatchNorm2d: 2-5                  [1, 64, 224, 224]         128\n",
              "│    └─ReLU: 2-6                         [1, 64, 224, 224]         --\n",
              "│    └─MaxPool2d: 2-7                    [1, 64, 112, 112]         --\n",
              "├─Sequential: 1-3                        [1, 128, 112, 112]        --\n",
              "│    └─Conv2d: 2-8                       [1, 128, 112, 112]        73,856\n",
              "│    └─BatchNorm2d: 2-9                  [1, 128, 112, 112]        256\n",
              "│    └─ReLU: 2-10                        [1, 128, 112, 112]        --\n",
              "├─Sequential: 1-4                        [1, 128, 56, 56]          --\n",
              "│    └─Conv2d: 2-11                      [1, 128, 112, 112]        147,584\n",
              "│    └─BatchNorm2d: 2-12                 [1, 128, 112, 112]        256\n",
              "│    └─ReLU: 2-13                        [1, 128, 112, 112]        --\n",
              "│    └─MaxPool2d: 2-14                   [1, 128, 56, 56]          --\n",
              "├─Sequential: 1-5                        [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-15                      [1, 256, 56, 56]          295,168\n",
              "│    └─BatchNorm2d: 2-16                 [1, 256, 56, 56]          512\n",
              "│    └─ReLU: 2-17                        [1, 256, 56, 56]          --\n",
              "├─Sequential: 1-6                        [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-18                      [1, 256, 56, 56]          590,080\n",
              "│    └─BatchNorm2d: 2-19                 [1, 256, 56, 56]          512\n",
              "│    └─ReLU: 2-20                        [1, 256, 56, 56]          --\n",
              "├─Sequential: 1-7                        [1, 256, 28, 28]          --\n",
              "│    └─Conv2d: 2-21                      [1, 256, 56, 56]          590,080\n",
              "│    └─BatchNorm2d: 2-22                 [1, 256, 56, 56]          512\n",
              "│    └─ReLU: 2-23                        [1, 256, 56, 56]          --\n",
              "│    └─MaxPool2d: 2-24                   [1, 256, 28, 28]          --\n",
              "├─Sequential: 1-8                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-25                      [1, 512, 28, 28]          1,180,160\n",
              "│    └─BatchNorm2d: 2-26                 [1, 512, 28, 28]          1,024\n",
              "│    └─ReLU: 2-27                        [1, 512, 28, 28]          --\n",
              "├─Sequential: 1-9                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-28                      [1, 512, 28, 28]          2,359,808\n",
              "│    └─BatchNorm2d: 2-29                 [1, 512, 28, 28]          1,024\n",
              "│    └─ReLU: 2-30                        [1, 512, 28, 28]          --\n",
              "├─Sequential: 1-10                       [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-31                      [1, 512, 28, 28]          2,359,808\n",
              "│    └─BatchNorm2d: 2-32                 [1, 512, 28, 28]          1,024\n",
              "│    └─ReLU: 2-33                        [1, 512, 28, 28]          --\n",
              "│    └─MaxPool2d: 2-34                   [1, 512, 14, 14]          --\n",
              "├─Sequential: 1-11                       [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-35                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─BatchNorm2d: 2-36                 [1, 512, 14, 14]          1,024\n",
              "│    └─ReLU: 2-37                        [1, 512, 14, 14]          --\n",
              "├─Sequential: 1-12                       [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-38                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─BatchNorm2d: 2-39                 [1, 512, 14, 14]          1,024\n",
              "│    └─ReLU: 2-40                        [1, 512, 14, 14]          --\n",
              "├─Sequential: 1-13                       [1, 512, 7, 7]            --\n",
              "│    └─Conv2d: 2-41                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─BatchNorm2d: 2-42                 [1, 512, 14, 14]          1,024\n",
              "│    └─ReLU: 2-43                        [1, 512, 14, 14]          --\n",
              "│    └─MaxPool2d: 2-44                   [1, 512, 7, 7]            --\n",
              "├─Sequential: 1-14                       [1, 4096]                 --\n",
              "│    └─Dropout: 2-45                     [1, 25088]                --\n",
              "│    └─Linear: 2-46                      [1, 4096]                 102,764,544\n",
              "│    └─ReLU: 2-47                        [1, 4096]                 --\n",
              "├─Sequential: 1-15                       [1, 4096]                 --\n",
              "│    └─Dropout: 2-48                     [1, 4096]                 --\n",
              "│    └─Linear: 2-49                      [1, 4096]                 16,781,312\n",
              "│    └─ReLU: 2-50                        [1, 4096]                 --\n",
              "├─Sequential: 1-16                       [1, 1000]                 --\n",
              "│    └─Linear: 2-51                      [1, 1000]                 4,097,000\n",
              "==========================================================================================\n",
              "Total params: 138,365,992\n",
              "Trainable params: 138,365,992\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 15.48\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 216.83\n",
              "Params size (MB): 553.46\n",
              "Estimated Total Size (MB): 770.90\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ]
}